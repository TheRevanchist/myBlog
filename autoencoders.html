<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Starting with Tensorflow</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Revan @ Github</h1>
      <h2 class="project-tagline">A small simple blog, where I will explain the work I am doing in github</h2>
      <a href="https://github.com/TheRevanchist/myBlog" class="btn">View on GitHub</a>
      <a href="https://github.com/TheRevanchist/myBlog/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/TheRevanchist/myBlog/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="The curious case of Autoencoders" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Autoencoders</h3>
      
<p> Autoencoders have been one of those things that I've always heard about, but never used them. They seems to be quite big
  in the online learning (deep learning courses like the one from Hinton) and even in the books (I am looking at you, Goodellow)
  but at the same time, I have yet to find someone who has used them to do something useful (I am talking about the vanilla
  version, not the variational ones which are almost a different things). I've also heard that they are a form of unsupervised
  learning, a way of getting good representation of objects in an unsupervised way. So, I decided to implement a simple autoencoder
  and see what's the fuss about. I also thought that this might be a good Tensorflow exercise (I have been using PyTorch on the
  last few months, a library I am really loving). </p>
  
<p>  On its basic form, autoencoders are a simple concept. The idea here is to map the input to output, via a neural network. A
  simple choice of a neural network, might be a fully connected neural network, with a hidden layer and an output layer, where
  the number of units in the output layer is the same as tbe number of units (features) in the input layer. A visual 
  representation of an autoencoder looks: </p>
  
  <img src="autoencoder.png" alt="A simple autoencoder" align="middle" title="A simple autoencoder">
      
<p> Of course, knowing that going deeper is better, it is possible to not limite the network on a single hidden layer. You 
  can go as deep as you want. A simple two layer autoencoder looks: </p>
  
  <img src="autoencoders2.png" alt="A simple autoencoder" align="middle" title="A simple two layer autoencoder">
  
<p> In fact, it is possible to change the architecture of the network to something more interesting,
  like a convolutional neural network. While, I am not going to cover today the Convolutional Autoencoder, it is essentially
  the same thing as a normal CNN, but instead of putting a final layer where you do some classification, you put a final
  layer that maps input to output (here is the auto in the autoencoder). A lot of knowledge from the usual neural nets
  can be transferred here. Obviously, we are not interested in the output (after all, in the best case, it is the same as the input), but we might be interested in the hidden representation, and
  at times, the features there might be better features than the original ones. </p>
      
<p> Enough with words though, let's see how things go in the code. We are going to use the MNIST dataset for this
  experiment. First, we do the neccessary imports and load the dataset: </p>  
      
<xmp> 
from __future__ import division, print_function, absolute_import

import tensorflow as tf
import numpy as np
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
import numpy as np

# Import MNIST data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data", one_hot=True) </xmp> 
      
<p> Then, we define the parameters for the learning rate, number of epochs we are going to train the network, the batch size
  and finally we define a placeholder for the dropout. </p>

<xmp>      
# initialize some parameters needed during the training
learning_rate = 3e-4
training_epochs = 20
batch_size = 128
display_step = 1
examples_to_show = 10
keep_prob = tf.placeholder(tf.float32)  </xmp> 
      
<p> Next, we define the number of units for each layer. MNIST images size are 28 by 28, so we have 784 units in the input
  layer, we arbitrarily decided to have 2048 units in the first hidden layer and 256 neuros in the second hidden layer. The
  numbers here are totally arbitrary, and for best results, it is needed to tune these parameters using a validation set. Then,
  we define and initialize the weights and biases of the net. </p>
      
<xmp>
# initialize tensorflow graph and the weights and biases of the neural network
X = tf.placeholder("float", [None, number_input_layer])
regularizer = tf.contrib.layers.l2_regularizer(scale=0.00)

weights = {
    'encoder_h1': tf.get_variable('WE', shape=[number_input_layer, number_hidden_layer1], regularizer=regularizer),
    'encoder_h2': tf.get_variable('WE2', shape=[number_hidden_layer1, number_hidden_layer2], regularizer=regularizer),   
    'decoder_h1': tf.get_variable('WD', shape=[number_hidden_layer2, number_hidden_layer1], regularizer=regularizer),
    'decoder_h2': tf.get_variable('WD2', shape=[number_hidden_layer1, number_input_layer], regularizer=regularizer)
}

biases = {
    'encoder_b1': tf.Variable(tf.random_normal([number_hidden_layer1])),
    'encoder_b2': tf.Variable(tf.random_normal([number_hidden_layer2])),    
    'decoder_b1': tf.Variable(tf.random_normal([number_hidden_layer1])),
    'decoder_b2': tf.Variable(tf.random_normal([number_input_layer]))
  } </xmp>
      
      



    
