<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Starting with Tensorflow</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Revan @ Github</h1>
      <h2 class="project-tagline">A small simple blog, where I will explain the work I am doing in github</h2>
      <a href="https://github.com/TheRevanchist/myBlog" class="btn">View on GitHub</a>
      <a href="https://github.com/TheRevanchist/myBlog/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/TheRevanchist/myBlog/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h3>
<a id="The curious case of Autoencoders" class="anchor" href="#welcome-to-github-pages" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Autoencoders</h3>
      
<p> Autoencoders have been one of those things that I've always heard about, but never used them. They seems to be quite big
  in the online learning (deep learning courses like the one from Hinton) and even in the books (I am looking at you, Goodellow)
  but at the same time, I have yet to find someone who has used them to do something useful (I am talking about the vanilla
  version, not the variational ones which are almost a different things). I've also heard that they are a form of unsupervised
  learning, a way of getting good representation of objects in an unsupervised way. So, I decided to implement a simple autoencoder
  and see what's the fuss about. I also thought that this might be a good Tensorflow exercise (I have been using PyTorch on the
  last few months, a library I am really loving). </p>
  
<p>  On its basic form, autoencoders are a simple concept. The idea here is to map the input to output, via a neural network. A
  simple choice of a neural network, might be a fully connected neural network, with a hidden layer and an output layer, where
  the number of units in the output layer is the same as tbe number of units (features) in the input layer. A visual 
  representation of an autoencoder looks: </p>
  
  <img src="autoencoder.png" alt="A simple autoencoder" align="middle" title="A simple autoencoder">
      
<p> Of course, knowing that going deeper is better, it is possible to not limite the network on a single hidden layer. You 
  can go as deep as you want. A simple two layer autoencoder looks: </p>
  
  <img src="autoencoders2.png" alt="A simple autoencoder" align="middle" title="A simple two layer autoencoder">
  
<p> In fact, it is possible to change the architecture of the network to something more interesting,
  like a convolutional neural network. While, I am not going to cover today the Convolutional Autoencoder, it is essentially
  the same thing as a normal CNN, but instead of putting a final layer where you do some classification, you put a final
  layer that maps input to output (here is the auto in the autoencoder). A lot of knowledge from the usual neural nets
  can be transferred here. Obviously, we are not interested in the output (after all, in the best case, it is the same as the input), but we might be interested in the hidden representation, and
  at times, the features there might be better features than the original ones. </p>
      
<p> Enough with words though, let's see how things go in the code. We are going to use the MNIST dataset for this
  experiment. First, we do the neccessary imports and load the dataset: </p>  
      
<xmp> 
from __future__ import division, print_function, absolute_import

import tensorflow as tf
import numpy as np
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
import numpy as np

# Import MNIST data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("MNIST_data", one_hot=True) </xmp> 
      
<p> Then, we define the parameters for the learning rate, number of epochs we are going to train the network, the batch size
  and finally we define a placeholder for the dropout. </p>

<xmp>      
# initialize some parameters needed during the training
learning_rate = 3e-4
training_epochs = 20
batch_size = 128
display_step = 1
examples_to_show = 10
keep_prob = tf.placeholder(tf.float32)  </xmp> 
      
<p> Next, we define the number of units for each layer. MNIST images size are 28 by 28, so we have 784 units in the input
  layer, we arbitrarily decided to have 2048 units in the first hidden layer and 256 neuros in the second hidden layer. The
  numbers here are totally arbitrary, and for best results, it is needed to tune these parameters using a validation set. Then,
  we define and initialize the weights and biases of the net. We also define a l2 regularizer (the scale of it should
  be tuned using a validation set. </p>
      
<xmp>
# initialize tensorflow graph and the weights and biases of the neural network
X = tf.placeholder("float", [None, number_input_layer])
regularizer = tf.contrib.layers.l2_regularizer(scale=0.001)

weights = {
    'encoder_h1': tf.get_variable('WE', shape=[number_input_layer, number_hidden_layer1], regularizer=regularizer),
    'encoder_h2': tf.get_variable('WE2', shape=[number_hidden_layer1, number_hidden_layer2], regularizer=regularizer),   
    'decoder_h1': tf.get_variable('WD', shape=[number_hidden_layer2, number_hidden_layer1], regularizer=regularizer),
    'decoder_h2': tf.get_variable('WD2', shape=[number_hidden_layer1, number_input_layer], regularizer=regularizer)
}

biases = {
    'encoder_b1': tf.Variable(tf.random_normal([number_hidden_layer1])),
    'encoder_b2': tf.Variable(tf.random_normal([number_hidden_layer2])),    
    'decoder_b1': tf.Variable(tf.random_normal([number_hidden_layer1])),
    'decoder_b2': tf.Variable(tf.random_normal([number_input_layer]))
  } </xmp>
      
<p> Now, we go to the main part. We define a function for the encoder and the decoder. They are symmetric functions, where
  the decoder function can be thought as the inverse of the encoder function. An interesting thing that I found, is that
  it seems that it is hard to make autoencoders work with hyperbolic tangent and the ReLU (and ReLU-like) activation
  functions. I don't have a good explanation for it right now, it just seems to not work straightforward. There are papers
  that deal with the situtaion, and Yoshua Bengio's lab reported that they made autoencoders work with the ReLU function.
  Anyway, in the code, the encoder/decoder functions is quite simple, you multiply the vector of neurons with the matrix
  of weights, add the bias term and apply a nonlinear activation function. Seems simple. </p>
      
<xmp> 
# Building the encoder
def encoder(x):
    # Encoder Hidden layer with sigmoid activation #1
    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),
                                   biases['encoder_b1']))
    layer_1_batch_norm = tf.contrib.layers.batch_norm(layer_1)
    h_fc1_drop = tf.nn.dropout(layer_1_batch_norm, keep_prob)
    
    # Decoder Hidden layer with sigmoid activation #2
    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1_batch_norm, weights['encoder_h2']),
                                   biases['encoder_b2']))
    return layer_2
    
# Building the decoder
def decoder(x):
    # Encoder Hidden layer with sigmoid activation #1
    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),
                                   biases['decoder_b1']))

    layer_1_batch_norm = tf.contrib.layers.batch_norm(layer_1)
    h_fc1_drop = tf.nn.dropout(layer_1_batch_norm, keep_prob)


    # Decoder Hidden layer with sigmoid activation #2
    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1_batch_norm, weights['decoder_h2']),
                                   biases['decoder_b2']))
    return layer_2 </xmp>  
      
<p> We continue by defining the model, the loss function and the optimizer. For loss function, we choose the least square
  loss function. Considering that we are trying to predict values that can take place in a large range of values, the least
  square cost function makes perfect sense. Finally, for the optimizer, we choose the Adam optimizer (my default choice for
  everything in neural networks) with 3e-4 learning rate and the other default parameters. </p>
      
 <xmp>
# the model
encoder_op = encoder(X)
decoder_op = decoder(encoder_op)

y_hat = decoder_op
y = X

# define loss function and minimize it
loss = tf.reduce_mean(tf.pow(y - y_hat, 2))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

# initialize the variables and launch the graph
init = tf.global_variables_initializer() </xmp>
      
<p> All that remains is training the neural network. On Tensorflow, that means definind the session, number of epochs and
  finally providing the training data and the dropout parameter (which we defined as placeholders where we defined the
  graph. We also save the internal representation of the final encoder layer. </p>
      
<xmp>
with tf.Session() as sess:
    sess.run(init)
    total_batch = int(mnist.train.num_examples/batch_size)
    # Training cycle
    for epoch in range(training_epochs):
        # Loop over all batches
        for i in range(total_batch):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            # Run optimization op (backprop) and cost op (to get loss value)
            _, c = sess.run([optimizer, loss], feed_dict={X: batch_xs, keep_prob: 0.5})
        # Display logs per epoch step
        if epoch % display_step == 0:
            print("Epoch:", '%04d' % (epoch+1), "cost=", "{:.9f}".format(c))

    print("Optimization Finished!")
    
    # Applying encode and decode over test set
    encode_decode = sess.run(y_hat, feed_dict={X: mnist.test.images[:examples_to_show], keep_prob: 1})
    features = encoder_op.eval(feed_dict={X: batch_xs, keep_prob: 1}) </xmp>  
      
<p> We look at the visual representation of the net. If the output looks quite similar to the input, than we have done
  a good job. </p>
 
<img src="digits.jpg" alt="On the first row are the original images, on the second row are the reconstructed images" align="middle" title="On the first row are the original images, on the second row are the reconstructed images">  
      
<p> The results seem very good and you can hardly find the difference between the original images and the reconstructed ones.
  However, when I tried to do k-means clustering, I still found that the results were slightly better if we use the original pixels,
  rather than the features from the encoder network. This comes a bit as a surprise to me, but to be fair, autoencoders are
  not exactly like CNN where you can get great results without doing much work. I think that in order to get good results
  in this experiment, a convolutional autoencoder is needed. And even there, the results can be not as impressive as we
  would hope for, which at the moment makes the usefuleness of autoencoders a bit dubious. </p>
      
      
      
     
      
      
      
      
      



    
